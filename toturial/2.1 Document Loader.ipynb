{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eade836b",
   "metadata": {},
   "source": [
    "## 1. CSV\n",
    "csv是使用逗号分割值的文本文件，每一行都是一条数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0a47df1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Publication Year</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Great Adventure</td>\n",
       "      <td>Alex Reed</td>\n",
       "      <td>2010</td>\n",
       "      <td>Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Mystery of the Ancients</td>\n",
       "      <td>Maria Lopez</td>\n",
       "      <td>2015</td>\n",
       "      <td>Mystery</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Lost City</td>\n",
       "      <td>John Doe</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fantasy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Journey Through Time</td>\n",
       "      <td>Emily Clarke</td>\n",
       "      <td>2020</td>\n",
       "      <td>Science Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Secrets of the Ocean</td>\n",
       "      <td>David Smith</td>\n",
       "      <td>2021</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Book ID                    Title        Author  Publication Year  \\\n",
       "0        1      The Great Adventure     Alex Reed              2010   \n",
       "1        2  Mystery of the Ancients   Maria Lopez              2015   \n",
       "2        3            The Lost City      John Doe              2018   \n",
       "3        4     Journey Through Time  Emily Clarke              2020   \n",
       "4        5     Secrets of the Ocean   David Smith              2021   \n",
       "\n",
       "             Genre  \n",
       "0        Adventure  \n",
       "1          Mystery  \n",
       "2          Fantasy  \n",
       "3  Science Fiction  \n",
       "4      Documentary  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./example data/2.1 Document Loader.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5785b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "loader = CSVLoader(file_path=\"./example data/2.1 Document Loader.csv\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "794c684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Book ID: 1\\nTitle: The Great Adventure\\nAuthor: Alex Reed\\nPublication Year: 2010\\nGenre: Adventure' metadata={'source': './example data/2.1 Document Loader.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "# 返回一个文件列表，每一行都是一个Document\n",
    "# 默认把所有的数据都作为Source\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3841aa5c",
   "metadata": {},
   "source": [
    "### 指定某列作为Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9585444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Book ID: 1\\nTitle: The Great Adventure\\nAuthor: Alex Reed\\nPublication Year: 2010\\nGenre: Adventure' metadata={'source': 'The Great Adventure', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader(file_path=\"./example data/2.1 Document Loader.csv\",\n",
    "                   source_column=\"Title\")\n",
    "data = loader.load()\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a425bb6a4e182",
   "metadata": {},
   "source": [
    "## 2. File dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43710c222ded5074",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-18T01:58:41.463602Z",
     "start_time": "2024-02-18T01:58:41.451137Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# 默认使用UnstructuredLoader\n",
    "loader = DirectoryLoader(\"./example data/\", glob=\"*.csv\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513b5e1",
   "metadata": {},
   "source": [
    "### 加进度条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8417e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 108.48it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = DirectoryLoader(\"./example data/\", glob=\"*.csv\", show_progress=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a40b29c",
   "metadata": {},
   "source": [
    "### 使用其他加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "669ee357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Book ID: 1\\nTitle: The Great Adventure\\nAuthor: Alex Reed\\nPublication Year: 2010\\nGenre: Adventure' metadata={'source': 'example data/2.1 Document Loader.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "loader = DirectoryLoader(\"./example data/\", glob=\"*.csv\", loader_cls=CSVLoader)\n",
    "docs = loader.load()\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b315a1e",
   "metadata": {},
   "source": [
    "### 3. HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b2588e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\nModules\\n\\nRetrieval\\n\\nDocument loaders\\n\\nHTML\\n\\nHTML\\n\\nThe HyperText Markup Language or HTML is the standard markup language for documents designed to be displayed in a web browser.\\n\\nThis covers how to load HTML documents into a document format that we can use downstream.\\n\\nfrom\\n\\nlangchain_community\\n\\ndocument_loaders\\n\\nimport\\n\\nUnstructuredHTMLLoader\\n\\nloader\\n\\nUnstructuredHTMLLoader\\n\\n\"example_data/fake-content.html\"\\n\\ndata\\n\\nloader\\n\\nload\\n\\ndata\\n\\n[Document(page_content=\\'My First Heading\\\\n\\\\nMy first paragraph.\\', lookup_str=\\'\\', metadata={\\'source\\': \\'example_data/fake-content.html\\'}, lookup_index=0)]\\n\\nLoading HTML with BeautifulSoup4\\u200b\\n\\nWe can also use BeautifulSoup4 to load HTML documents using the BSHTMLLoader.  This will extract the text from the HTML into page_content, and the page title as title into metadata.\\n\\nfrom\\n\\nlangchain_community\\n\\ndocument_loaders\\n\\nimport\\n\\nBSHTMLLoader\\n\\nloader\\n\\nBSHTMLLoader\\n\\n\"example_data/fake-content.html\"\\n\\ndata\\n\\nloader\\n\\nload\\n\\ndata\\n\\n[Document(page_content=\\'\\\\n\\\\nTest Title\\\\n\\\\n\\\\nMy First Heading\\\\nMy first paragraph.\\\\n\\\\n\\\\n\\', metadata={\\'source\\': \\'example_data/fake-content.html\\', \\'title\\': \\'Test Title\\'})]\\n\\nPreviousFile Directory\\n\\nNextJSON', metadata={'source': './example data/2.1 Langchain.html'})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "loader = UnstructuredHTMLLoader(\"./example data/2.1 Langchain.html\")\n",
    "data = loader.load()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695e2505",
   "metadata": {},
   "source": [
    "### 使用BeautifulSoup4加载HTML\n",
    "可以提取文本到page_content中，并将页面标题提取到metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d2808fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './example data/2.1 Langchain.html',\n",
       " 'title': 'HTML | 🦜️🔗 Langchain'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "loader = BSHTMLLoader(\"./example data/2.1 Langchain.html\")\n",
    "data = loader.load()\n",
    "data[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf1be5b",
   "metadata": {},
   "source": [
    "## 4. PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db49520e",
   "metadata": {},
   "source": [
    "### PyPDF\n",
    "每页pdf都会被加载为一个Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed9e6a40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Problem Chosen\\nD2024\\nMCM/ICM\\nSummary SheetTeam Control Number\\n2417262\\nGL-PaD: Great Lakes’ Predictor-and-Decision maker\\nSummary\\nThis paper focuses on optimizing water level in the Great Lakes. It aims to coordi-\\nnate the interests of stakeholders and develop a better water level management plan by\\ncombining environmental factors and historical data. The superiority of the algorithm is\\nverified through simulation.\\nFor requirement 1 , we propose a model to evaluate the interests of each stakeholder,\\nusing historical water level data to categorize them into primary and secondary groups.\\nAdditionally, the lakes are classified based on water level fluctuation magnitude, lead-\\ning to tailored optimization strategies for each. The established water levels for the five\\nlakes at 183.35, 176.33, 175.10, 174.28, and 74.82 meters serve as benchmarks for future\\noptimization control strategies.\\nFor requirement 2 , we presents a joint differential predictor and MPC decision-\\nmaker model, modified for iterative optimization via machine learning . Trained on 2002-\\n2022 data, the model achieved convergence and outperformed the traditional MPC deci-\\nsion model in water level control . It consistently maintained optimal water levels across\\nthe Great Lakes, within a defined error margin.\\nFor requirement 3 , we follows the model from requirement 2, retaining its structure\\nin the prediction phase. Inputs like 2017’s actual rainfall were used to simulate water\\nlevels under the dam decision model. Comparing these simulations with actual 2017 wa-\\nter levels shows our algorithm effectively lowered lake levels in the watershed, aligning\\nthem more closely with optimal levels. These results confirm the model’s effectiveness in\\nextreme weather and its lower sensitivity to specific environmental variables.\\nFor requirement 4 , we assesses the model’s sensitivity to environmental factors using\\nthe Sobol algorithm via the SAlib Python library. The algorithm separates the impact of\\nenvironmental variables into two measures: a main effect index for singular impacts and\\na total effect index for combined effects. Utilizing the Monte Carlo method, numerous\\nindependent, normally distributed samples were generated in a hypercube. Findings\\nshow that tributary inflow and ice stagnation rate significantly influence the model, more\\nso than other variables. Moreover, simultaneous changes in multiple variables have a\\ngreater impact on the model than individual variable changes.\\nFor requirement 5 , we analyze stakeholders around Lake Ontario and the St. Lawrence\\nRiver, evaluating their importance based on economic, ecological, and sensitivity aspects.\\nThe analysis not only adapt optimal water levels for each season, but also incorporates the\\ninfluence of seasonal changes in river dynamics and spring ice melt on the St. Lawrence\\nRiver. This leads to a comprehensive, multi-objective optimization model, which is vali-\\ndated through training and analysis, affirming the effectiveness of our control strategy.\\nKeywords : Water level control; Predictor; Desicion maker; Machine learning', metadata={'source': './example data/2417262.pdf', 'page': 0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./example data/2417262.pdf\")\n",
    "data = loader.load()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb3837d",
   "metadata": {},
   "source": [
    "#### 使用PyPDF提取图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c62d503a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LayoutParser : A Uniﬁed Toolkit for DL-Based DIA 5\n",
      "Table 1: Current layout detection models in the LayoutParser model zoo\n",
      "Dataset Base Model1Large Model Notes\n",
      "PubLayNet [38] F / M M Layouts of modern scientiﬁc documents\n",
      "PRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\n",
      "Newspaper [17] F - Layouts of scanned US newspapers from the 20th century\n",
      "TableBank [18] F F Table region on modern scientiﬁc and business document\n",
      "HJDataset [31] F / M - Layouts of history Japanese documents\n",
      "1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\n",
      "vs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\n",
      "backbones [ 13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [ 28] (F) and Mask\n",
      "R-CNN [ 12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\n",
      "using the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\n",
      "zoo in coming months.\n",
      "layout data structures , which are optimized for eﬃciency and versatility. 3) When\n",
      "necessary, users can employ existing or customized OCR models via the uniﬁed\n",
      "API provided in the OCR module . 4)LayoutParser comes with a set of utility\n",
      "functions for the visualization and storage of the layout data. 5) LayoutParser\n",
      "is also highly customizable, via its integration with functions for layout data\n",
      "annotation and model training . We now provide detailed descriptions for each\n",
      "component.\n",
      "3.1 Layout Detection Models\n",
      "InLayoutParser , a layout model takes a document image as an input and\n",
      "generates a list of rectangular boxes for the target content regions. Diﬀerent\n",
      "from traditional methods, it relies on deep convolutional neural networks rather\n",
      "than manually curated rules to identify content regions. It is formulated as an\n",
      "object detection problem and state-of-the-art models like Faster R-CNN [ 28] and\n",
      "Mask R-CNN [ 12] are used. This yields prediction results of high accuracy and\n",
      "makes it possible to build a concise, generalized interface for layout detection.\n",
      "LayoutParser , built upon Detectron2 [ 35], provides a minimal API that can\n",
      "perform layout detection with only four lines of code in Python:\n",
      "1import layoutparser as lp\n",
      "2image = cv2. imread (\" image_file \") # load images\n",
      "3model = lp. Detectron2LayoutModel (\n",
      "4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\n",
      "5layout = model . detect ( image )\n",
      "LayoutParser provides a wealth of pre-trained model weights using various\n",
      "datasets covering diﬀerent languages, time periods, and document types. Due to\n",
      "domain shift [ 7], the prediction performance can notably drop when models are ap-\n",
      "plied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\n",
      "document structures and layouts vary greatly in diﬀerent domains, it is important\n",
      "to select models trained on a dataset similar to the test samples. A semantic syntax\n",
      "is used for initializing the model weights in LayoutParser , using both the dataset\n",
      "name and model name lp://<dataset-name>/<model-architecture-name> .\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)\n",
    "pages = loader.load()\n",
    "print(pages[4].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2476ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
